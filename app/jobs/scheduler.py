# app/jobs/scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime, timedelta
from sqlalchemy import select
from sqlalchemy.orm import Session
from app.database import SessionLocal
from app.models import Fuente, Usuario
from app.scraper.generic import GenericScraper
from app.scraper.base import BaseScraper
from app.services.source_service import mark_scraped
import asyncio
import logging
from app.config import settings
import smtplib
from email.message import EmailMessage

log = logging.getLogger(__name__)

_scheduler: AsyncIOScheduler | None = None

async def _scrape_fuentes_usuario(db: Session, usuario_id: int):
    """Scrapea las fuentes permitidas para un usuario espec√≠fico seg√∫n su plan."""
    try:
        usuario = db.query(Usuario).filter(Usuario.id == usuario_id).first()
        if not usuario or not usuario.activo:
            log.warning(f"Usuario {usuario_id} no encontrado o inactivo")
            return

        # Obtener scraper base para verificar l√≠mites
        scraper_base = BaseScraper()
        fuentes_permitidas = scraper_base.obtener_fuentes_permitidas(db, usuario_id)
        
        if not fuentes_permitidas:
            log.info(f"üîí Usuario {usuario.email} no tiene fuentes permitidas para scrapear")
            return

        plan_info = "Premium" if usuario.plan == "premium" else f"Gratis ({len(fuentes_permitidas)}/{usuario.max_fuentes or 3} fuentes)"
        log.info(f"üîç Scrapeando para {usuario.email} ({plan_info}): {len(fuentes_permitidas)} fuentes")

        for fuente in fuentes_permitidas:
            try:
                log.info(f"üì∞ Scrapeando: {fuente.nombre} - {fuente.url_listado}")
                scraper = GenericScraper(fuente.url_listado)
                articulos = scraper.scrape_and_store()
                
                # Marcar como scrapeada
                mark_scraped(db, fuente.id)
                
                log.info(f"‚úÖ {fuente.nombre}: {len(articulos)} art√≠culos procesados")
                
            except Exception as e:
                log.error(f"‚ùå Error scrapeando {fuente.nombre}: {e}")
                continue

    except Exception as e:
        log.error(f"üí• Error en scraping para usuario {usuario_id}: {e}")

async def _scrape_all_users():
    """Scraping autom√°tico para todos los usuarios activos respetando l√≠mites de planes."""
    print(f"üöÄ Iniciando scraping autom√°tico - {datetime.now()}")
    
    db = SessionLocal()
    try:
        # Obtener todos los usuarios activos
        usuarios = db.query(Usuario).filter(Usuario.activo == True).all()
        print(f"üë• Usuarios activos: {len(usuarios)}")
        
        # Scrapear para cada usuario
        for usuario in usuarios:
            try:
                await _scrape_fuentes_usuario(db, usuario.id)
                print(f"‚úÖ Scraping completado para usuario: {usuario.email}")
            except Exception as e:
                print(f"‚ùå Error con usuario {usuario.email}: {e}")
                continue
        
        print(f"üéØ Scraping autom√°tico completado - {datetime.now()}")
        
    except Exception as e:
        print(f"üí• Error general en scraping autom√°tico: {e}")
    finally:
        db.close()


def run_trial_reminder_now():
    """Ejecuta la misma l√≥gica del job `trial_reminder_job` de forma s√≠ncrona y devuelve resumen.

    Esta funci√≥n es p√∫blica a nivel de m√≥dulo para que otros m√≥dulos (ej. web) puedan llamarla.
    """
    db = SessionLocal()
    summary = {"checked": 0, "candidates": 0, "sent": 0, "errors": []}
    try:
        now = datetime.utcnow()
        remind_date = now + timedelta(days=3)
        candidats = db.query(Usuario).filter(
            Usuario.plan == "premium",
            Usuario.plan_trial_expires.isnot(None),
            Usuario.plan_trial_expires <= remind_date,
            Usuario.plan_trial_expires > now,
            Usuario.plan_trial_reminder_sent.is_(None)
        ).all()

        summary["checked"] = 1
        summary["candidates"] = len(candidats)

        for u in candidats:
            try:
                subject = "Tu trial Premium expira en 3 d√≠as"
                body = f"Hola {u.nombre or u.email},\n\nTu trial Premium expirar√° el {u.plan_trial_expires}.\nSi deseas continuar con Premium, realiza la renovaci√≥n antes de esa fecha.\n\nSaludos,\nEquipo NexNews"

                sent = False
                if getattr(settings, 'SMTP_HOST', None) and getattr(settings, 'EMAIL_ENABLED', False):
                    try:
                        msg = EmailMessage()
                        msg['Subject'] = subject
                        msg['From'] = getattr(settings, 'SMTP_FROM', 'noreply@nexnews.com')
                        msg['To'] = u.email
                        msg.set_content(body)

                        with smtplib.SMTP(getattr(settings, 'SMTP_HOST'), getattr(settings, 'SMTP_PORT', 25)) as smtp:
                            if getattr(settings, 'SMTP_STARTTLS', False):
                                smtp.starttls()
                            if getattr(settings, 'SMTP_USER', None):
                                smtp.login(getattr(settings, 'SMTP_USER'), getattr(settings, 'SMTP_PASSWORD'))
                            smtp.send_message(msg)
                        sent = True
                    except Exception as e:
                        log.error(f"‚ùå Error sending SMTP reminder to {u.email}: {e}")

                if not sent:
                    log.info(f"[TRIAL-REMINDER] To: {u.email} | Subject: {subject} | Body: {body}")

                u.plan_trial_reminder_sent = datetime.utcnow()
                db.add(u)
                db.commit()
                summary["sent"] += 1
            except Exception as e:
                db.rollback()
                log.error(f"‚ùå Error processing reminder for {u.email}: {e}")
                summary["errors"].append(str(e))
    except Exception as e:
        log.error(f"üí• Error running trial reminders now: {e}")
        summary["errors"].append(str(e))
    finally:
        db.close()
    return summary

def _scrape_all_sources_legacy():
    """Funci√≥n legacy para compatibilidad (sin l√≠mites de usuario)"""
    print(f"üöÄ Iniciando scraping legacy - {datetime.now()}")
    db = SessionLocal()
    try:
        fuentes = db.scalars(select(Fuente).where(Fuente.habilitada == True)).all()  # noqa: E712
        print(f"üì∞ Fuentes habilitadas: {len(fuentes)}")
        
        for f in fuentes:
            try:
                print(f"üîç Scrapeando: {f.url_listado}")
                scraper = GenericScraper(f.url_listado)
                articulos = scraper.scrape_and_store()
                print(f"‚úÖ {f.url_listado}: {len(articulos)} art√≠culos procesados")
                mark_scraped(db, f.id)
            except Exception as e:
                print(f"‚ùå Error con {f.url_listado}: {e}")
    finally:
        db.close()
    print(f"üéØ Scraping legacy completado - {datetime.now()}")

def start_scheduler():
    """Inicia el scheduler con scraping autom√°tico que respeta l√≠mites de planes."""
    global _scheduler
    if _scheduler:
        return
    
    _scheduler = AsyncIOScheduler()
    
    # ‚úÖ SCRAPING AUTOM√ÅTICO CADA 2 HORAS (RESPETANDO L√çMITES)
    @_scheduler.scheduled_job("interval", minutes=120, id="scraping_automatico")
    def periodic_scrape():
        """Trabajo programado que respeta l√≠mites por usuario."""
        try:
            # Ejecutar en el event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # Si el loop est√° corriendo, crear task
                asyncio.create_task(_scrape_all_users())
            else:
                # Si no, ejecutar directamente
                loop.run_until_complete(_scrape_all_users())
        except Exception as e:
            log.error(f"üí• Error en job programado: {e}")

    # ‚úÖ SCRAPING R√ÅPIDO CADA 30 MINUTOS (SOLO PREMIUM) - OPCIONAL
    @_scheduler.scheduled_job("interval", minutes=30, id="scraping_rapido")
    def periodic_fast_scrape():
        """Scraping r√°pido cada 30 minutos solo para usuarios premium."""
        try:
            db = SessionLocal()
            try:
                usuarios_premium = db.query(Usuario).filter(
                    Usuario.activo == True,
                    Usuario.plan == "premium"
                ).all()
                
                if usuarios_premium:
                    log.info(f"üöÄ Scraping r√°pido para {len(usuarios_premium)} usuarios premium")
                    
                    for usuario in usuarios_premium:
                        asyncio.create_task(_scrape_fuentes_usuario(db, usuario.id))
                        
            finally:
                db.close()
                
        except Exception as e:
            log.error(f"üí• Error en scraping r√°pido: {e}")

    _scheduler.start()
    # ‚úÖ JOB ADICIONAL: Verificar trials expirados cada 1 hora
    @_scheduler.scheduled_job("interval", minutes=60, id="check_trials_expiry")
    def periodic_check_trials():
        try:
            db = SessionLocal()
            try:
                now = datetime.utcnow()
                expired = db.query(Usuario).filter(
                    Usuario.plan == "premium",
                    Usuario.plan_trial_expires.isnot(None),
                    Usuario.plan_trial_expires <= now
                ).all()
                if expired:
                    log.info(f"üîî Downgrading {len(expired)} users with expired trials")
                for u in expired:
                    try:
                        log.info(f"üîÑ Downgrading user {u.email} - trial expired {u.plan_trial_expires}")
                        u.plan = "gratis"
                        u.max_fuentes = 3
                        u.max_noticias_mes = 300
                        u.max_posts_social_mes = 500
                        u.plan_trial_start = None
                        u.plan_trial_expires = None
                        db.add(u)
                        db.commit()
                    except Exception as e:
                        db.rollback()
                        log.error(f"‚ùå Error downgrading user {u.email}: {e}")
            finally:
                db.close()
        except Exception as e:
            log.error(f"üí• Error checking trials expiry: {e}")
    
    # ‚úÖ JOB ADICIONAL: Enviar recordatorios de trial 3 d√≠as antes de expirar
    @_scheduler.scheduled_job("interval", minutes=60, id="trial_reminder_job")
    def periodic_trial_reminder():
        try:
            db = SessionLocal()
            try:
                now = datetime.utcnow()
                remind_date = now + timedelta(days=3)
                # Buscar usuarios con trial que expira dentro de ~3 d√≠as y que no han recibido recordatorio
                candidats = db.query(Usuario).filter(
                    Usuario.plan == "premium",
                    Usuario.plan_trial_expires.isnot(None),
                    Usuario.plan_trial_expires <= remind_date,
                    Usuario.plan_trial_expires > now,
                    Usuario.plan_trial_reminder_sent.is_(None)
                ).all()

                if not candidats:
                    return

                for u in candidats:
                    try:
                        # Preparar mensaje
                        subject = "Tu trial Premium expira en 3 d√≠as"
                        body = f"Hola {u.nombre or u.email},\n\nTu trial Premium expirar√° el {u.plan_trial_expires}.\nSi deseas continuar con Premium, realiza la renovaci√≥n antes de esa fecha.\n\nSaludos,\nEquipo NexNews"

                        sent = False
                        if getattr(settings, 'SMTP_HOST', None):
                            try:
                                msg = EmailMessage()
                                msg['Subject'] = subject
                                msg['From'] = getattr(settings, 'SMTP_FROM', 'noreply@nexnews.com')
                                msg['To'] = u.email
                                msg.set_content(body)

                                with smtplib.SMTP(getattr(settings, 'SMTP_HOST'), getattr(settings, 'SMTP_PORT', 25)) as smtp:
                                    if getattr(settings, 'SMTP_STARTTLS', False):
                                        smtp.starttls()
                                    if getattr(settings, 'SMTP_USER', None):
                                        smtp.login(getattr(settings, 'SMTP_USER'), getattr(settings, 'SMTP_PASSWORD'))
                                    smtp.send_message(msg)
                                sent = True
                            except Exception as e:
                                log.error(f"‚ùå Error sending SMTP reminder to {u.email}: {e}")

                        if not sent:
                            log.info(f"[TRIAL-REMINDER] To: {u.email} | Subject: {subject} | Body: {body}")

                        # Marcar recordatorio enviado
                        u.plan_trial_reminder_sent = datetime.utcnow()
                        db.add(u)
                        db.commit()
                    except Exception as e:
                        db.rollback()
                        log.error(f"‚ùå Error processing reminder for {u.email}: {e}")
            finally:
                db.close()
        except Exception as e:
            log.error(f"üí• Error in trial reminder job: {e}")
    
        # NOTE: run_trial_reminder_now existe tambi√©n como funci√≥n p√∫blica definida m√°s arriba
    print("=" * 60)
    print("‚úÖ SCHEDULER INICIADO")
    print("üìç Scraping autom√°tico cada 2 horas (todos los usuarios)")
    print("üìç Scraping r√°pido cada 30 minutos (solo premium)")
    print("üîí Respetando l√≠mites de planes:")
    print("   ‚Ä¢ Gratis: 3 fuentes m√°ximas")
    print("   ‚Ä¢ Premium: Fuentes ilimitadas")
    print("=" * 60)

def stop_scheduler():
    """Detiene el scheduler."""
    global _scheduler
    if _scheduler:
        _scheduler.shutdown()
        _scheduler = None
        print("üõë Scheduler detenido")

async def scrapear_usuario_manual(usuario_id: int):
    """Funci√≥n para scraping manual de un usuario espec√≠fico."""
    db = SessionLocal()
    try:
        await _scrape_fuentes_usuario(db, usuario_id)
    finally:
        db.close()

async def scrapear_fuente_manual(usuario_id: int, fuente_id: int) -> bool:
    """Funci√≥n para scraping manual de una fuente espec√≠fica con verificaci√≥n de permisos."""
    db = SessionLocal()
    try:
        scraper_base = BaseScraper()
        
        # Verificar permisos primero
        if not scraper_base.verificar_limite_fuentes_usuario(db, usuario_id, fuente_id):
            log.error(f"Usuario {usuario_id} no tiene permiso para scrapear fuente {fuente_id}")
            return False
        
        # Obtener la fuente
        fuente = db.query(Fuente).filter(Fuente.id == fuente_id).first()
        if not fuente:
            log.error(f"Fuente {fuente_id} no encontrada")
            return False
        
        # Scrapear la fuente
        try:
            log.info(f"üîç Scrapeando manualmente: {fuente.nombre}")
            scraper = GenericScraper(fuente.url_listado)
            articulos = scraper.scrape_and_store()
            mark_scraped(db, fuente.id)
            log.info(f"‚úÖ {fuente.nombre}: {len(articulos)} art√≠culos procesados")
            return True
            
        except Exception as e:
            log.error(f"‚ùå Error scrapeando {fuente.nombre}: {e}")
            return False
            
    finally:
        db.close()